{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "#import time\n",
    "#import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3', \n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proxy():\n",
    "    proxy_url = 'https://www.socks-proxy.net/'\n",
    "    proxy_list = []\n",
    "    headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3', \n",
    "               'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.current_proxies = {}\n",
    "        page = requests.get(self.proxy_url, headers=self.headers)\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            trs_list = soup.html.body.find('tbody').findAll('tr')\n",
    "            for tr in trs_list:\n",
    "                tds = tr.findAll('td')\n",
    "                if tds[6].text.lower() == 'yes':\n",
    "                    self.proxy_list.append((tds[4].text.lower(), f'{tds[0].text}:{tds[1].text}'))\n",
    "        \n",
    "\n",
    "    def change_proxy(self, num_tries=3):\n",
    "        tries = 0\n",
    "        while tries < num_tries:\n",
    "            tries += 1\n",
    "            proxy = random.choice(self.proxy_list)\n",
    "            try:\n",
    "                proxies={'http' : f'{proxy[0]}://{proxy[1]}',\n",
    "                         'https' : f'{proxy[0]}://{proxy[1]}'}\n",
    "                page = requests.get('https://hh.ru', headers=self.headers, proxies=proxies)\n",
    "                if page.status_code == 200:\n",
    "                    self.current_proxies = proxies\n",
    "                    break\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                continue\n",
    "                \n",
    "    @property\n",
    "    def get_proxy(self):\n",
    "        return self.current_proxies\n",
    "    \n",
    "    def get_proxy_list(self):\n",
    "        return self.proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurrencyExchange():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.usd_rate = 1\n",
    "        self.eur_rate = 1\n",
    "        self.update_rates_online()\n",
    "        \n",
    "    def update_rates_online(self):\n",
    "        cbr_page = requests.get('http://www.cbr.ru', headers=headers)\n",
    "        if cbr_page.status_code == 200:\n",
    "            soup = BeautifulSoup(cbr_page.text, 'lxml')\n",
    "            table = soup.findAll('tbody')[2].findAll('tr')\n",
    "            rates = []\n",
    "            for tr in table[1:]:\n",
    "                trs = tr.findAll('td')\n",
    "                rates.append(float(trs[1].text.strip().split()[1].replace(',','.')))\n",
    "            self.usd_rate, self.eur_rate = rates\n",
    "            \n",
    "    @property\n",
    "    def usd(self):\n",
    "        return self.usd_rate\n",
    "    \n",
    "    @property\n",
    "    def eur(self):\n",
    "        return self.eur_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http': 'socks4://209.203.130.25:54321',\n",
       " 'https': 'socks4://209.203.130.25:54321'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy = Proxy()\n",
    "if not proxy.get_proxy:\n",
    "    proxy.change_proxy()\n",
    "proxy.get_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_request(url, headers, proxy=Proxy(), num_tries=3):\n",
    "    if not proxy.get_proxy:\n",
    "        proxy.change_proxy()\n",
    "    proxies = proxy.get_proxy\n",
    "    \n",
    "    tries = 0\n",
    "    while tries < num_tries:\n",
    "        tries += 1\n",
    "        try:\n",
    "            page = requests.get(url, headers=headers, proxies=proxies)\n",
    "            return page\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            proxy.change_proxy()\n",
    "            proxies = proxy.get_proxy\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proxy.change_proxy()\n",
    "#page = proxy_request('https://yandex.ru/internet', headers, proxy)\n",
    "#if page.status_code == 200:\n",
    "#    print('OK')\n",
    "#    soup = BeautifulSoup(page.text, 'lxml')\n",
    "#    current_ip = soup.find('li').findAll('div')[1].text\n",
    "#    print(f'Using proxy {current_ip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hh_jobs(job_string, proxy, pages=1):\n",
    "    \n",
    "    def make_url(job_string, page_num=0):\n",
    "        BASE_HH_URL = 'https://hh.ru/search/vacancy?area=1&st=searchVacancy&text='\n",
    "        url = ''\n",
    "        if page_num:\n",
    "            url = BASE_HH_URL + '+'.join(job_string.strip().split()) + '&page=' + str(page_num)\n",
    "        else:\n",
    "            url = BASE_HH_URL + '+'.join(job_string.strip().split())\n",
    "        return url\n",
    "    \n",
    "    print(f'Collecting {pages} pages from HeadHunter')   \n",
    "    vacancies = []\n",
    "    page_num = 0\n",
    "    \n",
    "    page = proxy_request(make_url(job_string), headers, proxy)\n",
    "    \n",
    "    while (page.status_code == 200) and (page_num < pages):\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        try:\n",
    "            tmp_list = soup.find('div', attrs = {'class' : 'vacancy-serp'}).findAll('div', attrs = {'class' : 'vacancy-serp-item'})\n",
    "            vacancies.extend(tmp_list)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        page_num += 1\n",
    "        page = proxy_request(make_url(job_string, page_num), headers, proxy)\n",
    "    \n",
    "    return vacancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superjob_jobs(job_string, proxy, pages=1):\n",
    "    \n",
    "    def make_url(job_string, page_num=0):\n",
    "        BASE_URL = r'https://www.superjob.ru/vacancy/search/?keywords={}&geo%5Bt%5D%5B0%5D=4'\n",
    "        url = ''\n",
    "        if page_num:\n",
    "            url = BASE_URL.format('%20'.join(job_string.strip().split())) + '&page=' + str(page_num + 1)\n",
    "        else:\n",
    "            url = BASE_URL.format('%20'.join(job_string.strip().split()))\n",
    "        return url\n",
    "    \n",
    "    print(f'Collecting {pages} pages from SuperJob')   \n",
    "    vacancies = []\n",
    "    page_num = 0\n",
    "    \n",
    "    page = proxy_request(make_url(job_string), headers, proxy)\n",
    "    \n",
    "    while (page.status_code == 200) and (page_num < pages):\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        try:\n",
    "            tmp_list = soup.findAll('div', attrs = {'class' : 'f-test-vacancy-item'})\n",
    "            vacancies.extend(tmp_list)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        page_num += 1\n",
    "        page = proxy_request(make_url(job_string, page_num), headers, proxy)\n",
    "    \n",
    "    return vacancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parce_hh_item(item):\n",
    "    \n",
    "    def get_compensation(compensation):\n",
    "        \n",
    "        min_salary = None\n",
    "        max_salary = None\n",
    "        currency = None\n",
    "        \n",
    "        temp = compensation.text.replace('\\xa0', '').split()\n",
    "        if len(temp) == 2:\n",
    "            min_salary, max_salary = temp[0].split('-')\n",
    "            currency = temp[1]\n",
    "        elif temp[0] == 'от':\n",
    "            min_salary = temp[1]\n",
    "            currency = temp[2]\n",
    "        elif temp[0] == 'до':\n",
    "            max_salary = temp[1]\n",
    "            currency = temp[2]\n",
    "        return min_salary, max_salary, currency\n",
    "    \n",
    "    item_dict = {}\n",
    "    \n",
    "    item_dict['source'] = 'HeadHunter'  \n",
    "    item_dict['premium'] = 'vacancy-serp-item_premium' in item.get('class')\n",
    "    item_dict['title'] = item.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-title'}).text\n",
    "    item_dict['vacancy_link'] = item.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-title'}).get('href')\n",
    "    item_dict['employer'] = item.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-employer'}).text.strip()\n",
    "    item_dict['employer_link'] = 'https://hh.ru' + \\\n",
    "        item.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-employer'}).get('href')\n",
    "    item_dict['location'] = item.find('span', attrs = {'data-qa' : 'vacancy-serp__vacancy-address'}).text\n",
    "    item_dict['responsibility'] = item.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy_snippet_responsibility'}).text\n",
    "    item_dict['requirement'] = item.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy_snippet_requirement'}).text\n",
    "    item_dict['date'] = item.find('span', attrs = {'data-qa' : 'vacancy-serp__vacancy-date'}).text.replace('\\xa0', ' ')\n",
    "    \n",
    "    compensation = item.find('div', attrs = {'class' : 'vacancy-serp-item__compensation'})\n",
    "    if compensation:\n",
    "        item_dict['min_salary'], item_dict['max_salary'], item_dict['currency'] = get_compensation(compensation)\n",
    "    \n",
    "    return item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parce_superjob_item(item):\n",
    "    \n",
    "    def get_compensation(compensation):\n",
    "        \n",
    "        currency_dict = {\n",
    "                36 : 'USD',\n",
    "                8364 : 'EUR',\n",
    "                8381 : 'руб.'\n",
    "            }\n",
    "        \n",
    "        min_salary = None\n",
    "        max_salary = None\n",
    "        currency = None\n",
    "        \n",
    "        if compensation != 'По договорённости':\n",
    "            currency = currency_dict[ord(compensation[-1])]\n",
    "            compensation = compensation[:-1]\n",
    "            if '—' in compensation:\n",
    "                min_salary, max_salary = compensation.split('—')            \n",
    "            else:\n",
    "                if compensation[:2] == 'от':\n",
    "                    min_salary = compensation[2:]\n",
    "                elif compensation[:2] == 'до':\n",
    "                    max_salary = compensation[2:]\n",
    "                else:\n",
    "                    min_salary = max_salary = compensation\n",
    "        return min_salary, max_salary, currency\n",
    "    \n",
    "    \n",
    "    item_dict = {}\n",
    "    \n",
    "    item_dict['source'] = 'SuperJob'\n",
    "    \n",
    "    a_tags = item.findAll('a')\n",
    "    item_dict['title'] = a_tags[-2].text\n",
    "    item_dict['vacancy_link'] = a_tags[-2].get('href')\n",
    "    item_dict['employer'] = a_tags[-1].text\n",
    "    item_dict['employer_link'] = 'https://www.superjob.ru/' + a_tags[-1].get('href')\n",
    "    span_tags = item.find('span', attrs = {'class' : 'f-test-text-company-item-location'}).findAll('span', recursive=False)\n",
    "    item_dict['location'] = span_tags[1].text\n",
    "    item_dict['date'] = span_tags[0].text\n",
    "    \n",
    "    description = item.findAll('span', attrs = {'class' : '_3mfro _9fXTd _2JVkc _2VHxz _15msI'})\n",
    "    for desc_item in description:\n",
    "        if desc_item.span.text == 'Должностные обязанности:':\n",
    "            item_dict['responsibility'] = desc_item.text[24:].strip()\n",
    "        if desc_item.span.text == 'Требования:':\n",
    "            item_dict['requirement'] = desc_item.text[11:].strip()\n",
    "    \n",
    "    \n",
    "    compensation = item.find('span', attrs = {'class' : 'f-test-text-company-item-salary'}).text.replace('\\xa0', '')\n",
    "    item_dict['min_salary'], item_dict['max_salary'], item_dict['currency'] = get_compensation(compensation)\n",
    "    \n",
    "    return item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacancies_all(job_string, num_pages=1):\n",
    "    \n",
    "    proxy = Proxy()\n",
    "    proxy.change_proxy()\n",
    "    \n",
    "    vacancies = []\n",
    "    \n",
    "    hh_jobs = get_hh_jobs(job_string, proxy, num_pages)\n",
    "    vacancies.extend([parce_hh_item(item) for item in hh_jobs])\n",
    "    \n",
    "    sj_jobs = get_superjob_jobs(job_string, proxy, num_pages)\n",
    "    vacancies.extend([parce_superjob_item(item) for item in sj_jobs])\n",
    "    \n",
    "    data = pd.DataFrame(vacancies, columns=['title', 'vacancy_link', 'employer', 'employer_link', \n",
    "                                            'source', 'premium', 'location', 'responsibility', 'requirement',\n",
    "                                            'date', 'min_salary', 'max_salary', 'currency'])\n",
    "    \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 1 pages from HeadHunter\n",
      "Collecting 1 pages from SuperJob\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 13 columns):\n",
      "title             22 non-null object\n",
      "vacancy_link      22 non-null object\n",
      "employer          22 non-null object\n",
      "employer_link     22 non-null object\n",
      "source            22 non-null object\n",
      "premium           20 non-null object\n",
      "location          22 non-null object\n",
      "responsibility    22 non-null object\n",
      "requirement       22 non-null object\n",
      "date              22 non-null object\n",
      "min_salary        8 non-null object\n",
      "max_salary        5 non-null object\n",
      "currency          8 non-null object\n",
      "dtypes: object(13)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data = get_vacancies_all('data science')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>vacancy_link</th>\n",
       "      <th>employer</th>\n",
       "      <th>employer_link</th>\n",
       "      <th>source</th>\n",
       "      <th>premium</th>\n",
       "      <th>location</th>\n",
       "      <th>responsibility</th>\n",
       "      <th>requirement</th>\n",
       "      <th>date</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://hh.ru/vacancy/35011897?query=data%20sc...</td>\n",
       "      <td>HeadHunter::Analytics/Data Science</td>\n",
       "      <td>https://hh.ru/employer/1455?dpt=hh-1455-ds</td>\n",
       "      <td>HeadHunter</td>\n",
       "      <td>True</td>\n",
       "      <td>Москва, Алексеевская</td>\n",
       "      <td>Статистический анализ и обработка данных. Пост...</td>\n",
       "      <td>Опыт реализации алгоритмов машинного обучения ...</td>\n",
       "      <td>16 декабря</td>\n",
       "      <td>200000</td>\n",
       "      <td>None</td>\n",
       "      <td>руб.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>https://hh.ru/vacancy/35033890?query=data%20sc...</td>\n",
       "      <td>ЗАО ОКБ</td>\n",
       "      <td>https://hh.ru/employer/2129243</td>\n",
       "      <td>HeadHunter</td>\n",
       "      <td>True</td>\n",
       "      <td>Москва, Третьяковская и еще 2</td>\n",
       "      <td>Построение и поддержка системы мониторинга и о...</td>\n",
       "      <td>Опыт работы аналитиком по данным в риск-подраз...</td>\n",
       "      <td>17 декабря</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst (Senior)</td>\n",
       "      <td>https://hh.ru/vacancy/33864288?query=data%20sc...</td>\n",
       "      <td>Сбербанк для экспертов</td>\n",
       "      <td>https://hh.ru/employer/3529?dpt=3529-3529-prof</td>\n",
       "      <td>HeadHunter</td>\n",
       "      <td>True</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Анализ бизнес требований и формулировка задачи...</td>\n",
       "      <td>Понимание математической статистики. Знание SQ...</td>\n",
       "      <td>13 декабря</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product Analyst</td>\n",
       "      <td>https://hh.ru/vacancy/32845591?query=data%20sc...</td>\n",
       "      <td>HeadHunter::Analytics/Data Science</td>\n",
       "      <td>https://hh.ru/employer/1455?dpt=hh-1455-ds</td>\n",
       "      <td>HeadHunter</td>\n",
       "      <td>True</td>\n",
       "      <td>Москва, Алексеевская</td>\n",
       "      <td>1. Собирать и анализировать данные о поведении...</td>\n",
       "      <td>Опыт построения аналитических моделей и тестир...</td>\n",
       "      <td>16 декабря</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Рекрутер (подбор d-people)/Team lead</td>\n",
       "      <td>https://hh.ru/vacancy/32426044?query=data%20sc...</td>\n",
       "      <td>Сбербанк для экспертов</td>\n",
       "      <td>https://hh.ru/employer/3529?dpt=3529-3529-prof</td>\n",
       "      <td>HeadHunter</td>\n",
       "      <td>True</td>\n",
       "      <td>Москва, Кутузовская и еще 1</td>\n",
       "      <td>Закрытие вакансий в направлении Data Science, ...</td>\n",
       "      <td>Опыт работы рекрутером/ менеджером по подбору ...</td>\n",
       "      <td>13 декабря</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "0                        Data Scientist   \n",
       "1                          Data Analyst   \n",
       "2                 Data Analyst (Senior)   \n",
       "3                       Product Analyst   \n",
       "4  Рекрутер (подбор d-people)/Team lead   \n",
       "\n",
       "                                        vacancy_link  \\\n",
       "0  https://hh.ru/vacancy/35011897?query=data%20sc...   \n",
       "1  https://hh.ru/vacancy/35033890?query=data%20sc...   \n",
       "2  https://hh.ru/vacancy/33864288?query=data%20sc...   \n",
       "3  https://hh.ru/vacancy/32845591?query=data%20sc...   \n",
       "4  https://hh.ru/vacancy/32426044?query=data%20sc...   \n",
       "\n",
       "                             employer  \\\n",
       "0  HeadHunter::Analytics/Data Science   \n",
       "1                             ЗАО ОКБ   \n",
       "2              Сбербанк для экспертов   \n",
       "3  HeadHunter::Analytics/Data Science   \n",
       "4              Сбербанк для экспертов   \n",
       "\n",
       "                                    employer_link      source premium  \\\n",
       "0      https://hh.ru/employer/1455?dpt=hh-1455-ds  HeadHunter    True   \n",
       "1                  https://hh.ru/employer/2129243  HeadHunter    True   \n",
       "2  https://hh.ru/employer/3529?dpt=3529-3529-prof  HeadHunter    True   \n",
       "3      https://hh.ru/employer/1455?dpt=hh-1455-ds  HeadHunter    True   \n",
       "4  https://hh.ru/employer/3529?dpt=3529-3529-prof  HeadHunter    True   \n",
       "\n",
       "                         location  \\\n",
       "0            Москва, Алексеевская   \n",
       "1  Москва, Третьяковская и еще 2    \n",
       "2                          Москва   \n",
       "3            Москва, Алексеевская   \n",
       "4    Москва, Кутузовская и еще 1    \n",
       "\n",
       "                                      responsibility  \\\n",
       "0  Статистический анализ и обработка данных. Пост...   \n",
       "1  Построение и поддержка системы мониторинга и о...   \n",
       "2  Анализ бизнес требований и формулировка задачи...   \n",
       "3  1. Собирать и анализировать данные о поведении...   \n",
       "4  Закрытие вакансий в направлении Data Science, ...   \n",
       "\n",
       "                                         requirement        date min_salary  \\\n",
       "0  Опыт реализации алгоритмов машинного обучения ...  16 декабря     200000   \n",
       "1  Опыт работы аналитиком по данным в риск-подраз...  17 декабря        NaN   \n",
       "2  Понимание математической статистики. Знание SQ...  13 декабря        NaN   \n",
       "3  Опыт построения аналитических моделей и тестир...  16 декабря        NaN   \n",
       "4  Опыт работы рекрутером/ менеджером по подбору ...  13 декабря        NaN   \n",
       "\n",
       "  max_salary currency  \n",
       "0       None     руб.  \n",
       "1        NaN      NaN  \n",
       "2        NaN      NaN  \n",
       "3        NaN      NaN  \n",
       "4        NaN      NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "руб.    8\n",
       "Name: currency, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.currency.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_change = CurrencyExchange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.5831, 69.6925)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_change.usd, cur_change.eur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
